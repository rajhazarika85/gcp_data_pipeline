from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from google.cloud import storage
import pandas as pd
import io

# ---- Config ----
GCS_BUCKET = "de-project-473910-marketing-data"  # ğŸ” your bucket name
GCS_FILE = "marketing/raw/marketing_data.csv"             # ğŸ” path to your CSV file

def read_and_print_csv():
    """Read CSV from GCS and print its contents."""
    client = storage.Client()
    bucket = client.bucket(GCS_BUCKET)
    blob = bucket.blob(GCS_FILE)

    data = blob.download_as_text()
    df = pd.read_csv(io.StringIO(data))

    print("âœ… File read successfully!")
    print("ğŸ“„ Columns:", list(df.columns))
    print("ğŸ“Š Data Preview:\n", df.head())

# ---- DAG Definition ----
with DAG(
    dag_id="gcs_pandas_dag",
    schedule_interval=None,  # Run manually
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["gcs", "pandas"],
) as dag:

    read_gcs_task = PythonOperator(
        task_id="read_and_print_csv",
        python_callable=read_and_print_csv,
    )

    read_gcs_task
